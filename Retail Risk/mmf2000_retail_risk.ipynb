{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from varclushi import VarClusHi\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, auc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn2pmml.decoration import CategoricalDomain\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "from sklearn2pmml.pipeline import PMMLPipeline\n",
    "from sklearn2pmml.tree.chaid import CHAIDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mmf_data2.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.3: Target Variable\n",
    "target_cols = [f\"t{month}\" for month in range(1, 13)]\n",
    "df[\"target\"] = df[target_cols].sum(axis=1).apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.4: Population Exclusions\n",
    "print(f\"number of customers in the population before exclusions: {df.shape[0]}\")\n",
    "df = df.drop(df[df[\"WIDELYHD\"] == \"Y\"].index).reset_index(drop=True)\n",
    "df = df.drop(df[df[\"deceased\"] > 0].index).reset_index(drop=True)\n",
    "print(f\"number of customers in the population after exclusions: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.5: Modeling Population\n",
    "default_counts = df[\"target\"].value_counts()\n",
    "print(\"total population\")\n",
    "print(\"number of default customers: \", default_counts[1])\n",
    "print(\"number of non-default customers: \", default_counts[0])\n",
    "print(\"default rate: \", default_counts[1]/df.shape[0])\n",
    "\n",
    "default_counts_by_timekey = df.groupby(\"TIME_KEY\").target.agg(['sum', 'count'])\n",
    "time_key_dict = {\n",
    "    15076: \"2014-01\",\n",
    "    15196: \"2014-04\",\n",
    "    15316: \"2014-07\",\n",
    "    15436: \"2014-10\"\n",
    "}\n",
    "for key in time_key_dict:\n",
    "    print(f\"\\n{time_key_dict[key]}\")\n",
    "    print(\"number of default customers: \", default_counts_by_timekey.loc[key, \"sum\"])\n",
    "    print(\"number of non-default customers: \", default_counts_by_timekey.loc[key, \"count\"] - default_counts_by_timekey.loc[key, \"sum\"])\n",
    "    print(\"default rate: \", default_counts_by_timekey.loc[key, \"sum\"] / default_counts_by_timekey.loc[key, \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.6: Explanatory Variables\n",
    "debit_cols = [\"debit_curr\"] + [f\"debit_prev{month}\" for month in range(1, 13)]\n",
    "credit_cols = [\"credit_curr\"] + [f\"credit_prev{month}\" for month in range(1, 13)]\n",
    "\n",
    "def get_linear_trend(row, t=np.arange(0, -13, -1)):\n",
    "    reg = LinearRegression().fit(t.reshape(-1, 1), row.to_numpy())\n",
    "    return reg.coef_[0]\n",
    "\n",
    "df[\"debit_max\"] = df[debit_cols].max(axis=1)\n",
    "df[\"debit_min\"] = df[debit_cols].min(axis=1)\n",
    "df[\"debit_avg\"] = df[debit_cols].mean(axis=1)\n",
    "df[\"debit_stdev\"] = df[debit_cols].std(axis=1)\n",
    "df[\"debit_trend\"] = df[debit_cols].apply(lambda row: get_linear_trend(row), axis=1)\n",
    "debit_new_cols = [\"debit_max\", \"debit_min\", \"debit_avg\", \"debit_stdev\", \"debit_trend\"]\n",
    "\n",
    "df[\"credit_max\"] = df[credit_cols].max(axis=1)\n",
    "df[\"credit_min\"] = df[credit_cols].min(axis=1)\n",
    "df[\"credit_avg\"] = df[credit_cols].mean(axis=1)\n",
    "df[\"credit_stdev\"] = df[credit_cols].std(axis=1)\n",
    "df[\"credit_trend\"] = df[credit_cols].apply(lambda row: get_linear_trend(row), axis=1)\n",
    "credit_new_cols = [\"credit_max\", \"credit_min\", \"credit_avg\", \"credit_stdev\", \"credit_trend\"]\n",
    "\n",
    "df[\"debit_credit_ratio\"] = df[\"debit_avg\"] / df[\"credit_avg\"]\n",
    "# monthly debit-to-credit ratio variables (variance too large)\n",
    "# df[\"debit_credit_ratio_curr\"] = df[\"debit_curr\"] / df[\"credit_curr\"]\n",
    "# ratio_cols = [\"debit_credit_ratio_avg\", \"debit_credit_ratio_curr\"]\n",
    "# for month in range(1, 13):\n",
    "#     df[f\"debit_credit_ratio{month}\"] = df[f\"debit_prev{month}\"] / df[f\"credit_prev{month}\"]\n",
    "#     ratio_cols.append(f\"debit_credit_ratio{month}\")\n",
    "\n",
    "new_cols = debit_new_cols + credit_new_cols + [\"debit_credit_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.7: Segmentation\n",
    "cluster_cols = [\n",
    "    'CUSTTYPE',\n",
    "    'HIGHRISK',\n",
    "    'STARTUP',\n",
    "    'TERMCUST',\n",
    "    'OLCUST',\n",
    "    'VISACUST',\n",
    "    'Out_of_Country',\n",
    "    'SPP_Group_1',\n",
    "    'SPP_Group_2',\n",
    "    'DOCTOR_DENTIST_IND',\n",
    "    'RESTAURANT_IND',\n",
    "    'TRUCKING_IND',\n",
    "    'RESTAURANT_OR_TRUCKING_IND',\n",
    "    'SPP_LISTED_IND',\n",
    "    'SPP_TARGETED_IND',\n",
    "    'SPP_LISTED_OR_TARGETED_IND'\n",
    "]\n",
    "\n",
    "cluster_df = df.loc[:, cluster_cols]\n",
    "for col in cluster_cols:\n",
    "    # skip numeric variables\n",
    "    if col == \"CUSTTYPE\":\n",
    "        cluster_df[\"operating_account\"] = cluster_df[col].map({\"O\": 1, \"T\": 0, \"B\": 1})\n",
    "        cluster_df[\"term_account\"] = cluster_df[col].map({\"O\": 0, \"T\": 1, \"B\": 1})\n",
    "    else:\n",
    "        cluster_df[col] = cluster_df[col].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "X_cluster = cluster_df.drop(\"CUSTTYPE\", axis=1).to_numpy()\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "for k in range(2, 15):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=2000).fit(X_cluster)\n",
    "    cluster_labels = kmeans.fit_predict(X_cluster)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "    # silhouette_avg = silhouette_score(X_cluster, cluster_labels)\n",
    "    # silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(2, 15), distortions)\n",
    "ax.set(xlabel=\"k\", ylabel='distortion')\n",
    "ax.set_title(\"Elbow Method to Find the Optimal k\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('media/kmeans.png')\n",
    "plt.show()\n",
    "\n",
    "# we select k = 5 as the optimal number of cluster\n",
    "clusterer = KMeans(n_clusters=5, random_state=2000).fit(X_cluster)\n",
    "cluster_labels = clusterer.fit_predict(X_cluster)\n",
    "df[\"cluster_num\"] = cluster_labels\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(np.arange(2, 15), silhouette_scores)\n",
    "# ax.set(xlabel=\"k\", ylabel='silhouette score')\n",
    "# ax.set_title(\"Silhouette Analysis to Find the Optimal k\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Scorecard Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2.1: Pre-Screening\n",
    "processed_cols = [col[4:] for col in df.columns if col.startswith(\"GRP_\")]\n",
    "benchmark_cols = [col for col in df.columns if col.startswith(\"benchmark\")]\n",
    "population_exclusion_cols = [\"WIDELYHD\", \"deceased\"]\n",
    "index_cols = [\"tu_seq_id\", \"TIME_KEY\"]\n",
    "constant_cols = list(df.loc[:, df.nunique() == 1].columns)\n",
    "# sparse_cols = list(df.loc[:, df.isna().sum() > 0.1*df.shape[0]].columns)\n",
    "sparse_cols = list(df.loc[:, df.isna().sum() > 0].columns)\n",
    "\n",
    "cols_to_ignore = sum(\n",
    "    [\n",
    "        processed_cols,\n",
    "        target_cols, [\"target\"],\n",
    "        benchmark_cols,\n",
    "        new_cols,\n",
    "        debit_cols,\n",
    "        credit_cols,\n",
    "        population_exclusion_cols,\n",
    "        index_cols,\n",
    "        constant_cols,\n",
    "        sparse_cols\n",
    "    ],\n",
    "    []\n",
    ")\n",
    "prescreened_cols = [col for col in df.columns if not (col.startswith(\"GRP_\") or col.startswith(\"WOE_\") or col in cols_to_ignore)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2.2: Univariate Screening\n",
    "def get_WOE_IV(df, col, preprocessed=False):\n",
    "    df_groupby = df.groupby(col).target.agg(['sum', 'count'])\n",
    "    df_groupby[\"f_B\"] = df_groupby[\"sum\"] / df.target.sum()  # distribution of default (\"bad\") customers\n",
    "    df_groupby[\"f_G\"] = (df_groupby[\"count\"] - df_groupby[\"sum\"]) / (df.shape[0] - df.target.sum())  # distribution of non-default (\"good\") customers\n",
    "    if preprocessed:\n",
    "        WOE_df = df[[col, \"WOE_\" + col.replace(\"GRP_\", \"\")]].drop_duplicates()\n",
    "        WOE_dict = dict(zip(WOE_df[col], WOE_df[\"WOE_\" + col.replace(\"GRP_\", \"\")]))\n",
    "        df_groupby[\"WOE\"] = df_groupby.index.to_series().map(WOE_dict)\n",
    "    else:\n",
    "        df_groupby[\"WOE\"] = np.log(df_groupby[\"f_G\"] / df_groupby[\"f_B\"])\n",
    "    df_groupby[\"IV\"] = (df_groupby[\"f_G\"] - df_groupby[\"f_B\"]) * df_groupby[\"WOE\"]\n",
    "    return df_groupby\n",
    "\n",
    "\n",
    "bins_dict = {\n",
    "    \"debit_max\": np.linspace(25000, 50000, 11),\n",
    "    \"debit_min\": [0, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 25000],\n",
    "    \"debit_avg\": np.linspace(10000, 40000, 11),\n",
    "    \"debit_stdev\": np.linspace(6000, 21000, 11),\n",
    "    \"debit_trend\": [-4000, -2400, -1600, -800, 0, 800, 1600, 2400, 4000],\n",
    "    \"credit_max\": [20000, 29000, 32000, 35000, 38000, 41000, 44000, 47000, 50000],\n",
    "    \"credit_min\": [0, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000, 25000],\n",
    "    \"credit_avg\": [10000, 16000, 19000, 22000, 25000, 28000, 31000, 34000, 37000, 40000],\n",
    "    \"credit_stdev\": [6500, 9500, 11000, 12500, 14000, 15500, 17000, 18500, 21500],\n",
    "    \"credit_trend\": [-4100, -2460, -1640, -820, 0, 820, 1640, 2460, 4100],\n",
    "    \"debit_credit_ratio\": [0.3, 0.54, 0.78, 1.02, 1.26, 1.5, 1.74, 1.98, 2.7]\n",
    "}\n",
    "\n",
    "for col in new_cols:\n",
    "    df[f\"GRP_{col}\"] = pd.cut(df[col], bins_dict[col], labels=np.arange(1, len(bins_dict[col])))\n",
    "df[\"GRP_TBSAT01\"] = pd.cut(df[\"TBSAT01\"], [-8, -1, 0, 1, 2, 3, 4, 5, 7, 10, 30], labels=np.arange(1, 11))\n",
    "df[\"WOE_TBSAT01\"] = df[\"GRP_TBSAT01\"].map(dict(zip(get_WOE_IV(df, \"GRP_TBSAT01\").index, get_WOE_IV(df, \"GRP_TBSAT01\").WOE)))\n",
    "df[\"GRP_cust_max_dlq_3mos\"] = pd.cut(df[\"cust_max_dlq_3mos\"], [-1, 0, 1, 2, 6], labels=np.arange(1, 5))\n",
    "df[\"WOE_cust_max_dlq_3mos\"] = df[\"GRP_cust_max_dlq_3mos\"].map(dict(zip(get_WOE_IV(df, \"GRP_cust_max_dlq_3mos\").index, get_WOE_IV(df, \"GRP_cust_max_dlq_3mos\").WOE)))\n",
    "\n",
    "# encoding categorical variables\n",
    "for col in prescreened_cols:\n",
    "    # skip numeric variables\n",
    "    if col in ['cust_max_dlq_3mos', 'agri_cust', 'TBSAT01']:\n",
    "        continue\n",
    "    elif col == \"CUSTTYPE\":\n",
    "        df[\"operating_account\"] = df[col].map({\"O\": 1, \"T\": 0, \"B\": 1})\n",
    "        df[\"term_account\"] = df[col].map({\"O\": 0, \"T\": 1, \"B\": 1})\n",
    "    else:\n",
    "        df[col] = df[col].map({\"Y\": 1, \"N\": 0})\n",
    "\n",
    "# for col in new_cols:\n",
    "#     df_groupby = get_WOE_IV(df, f\"GRP_{col}\")\n",
    "#     print(col)\n",
    "#     print(df_groupby)\n",
    "#     print(df_groupby.IV.sum())\n",
    "\n",
    "IV_dict = dict()\n",
    "GRP_cols = [col for col in df.columns if col.startswith(\"GRP_\")]  # this includes processed columns and new variables\n",
    "cols_to_verify_with_IV = prescreened_cols + GRP_cols + [\"operating_account\", \"term_account\"]\n",
    "for col in cols_to_verify_with_IV:\n",
    "    # these variables have already been processed into different columns\n",
    "    if col in [\"CUSTTYPE\", \"TBSAT01\", \"cust_max_dlq_3mos\"]:\n",
    "        continue\n",
    "    df_groupby = get_WOE_IV(df, col)\n",
    "    # IV_dict[col] = df_groupby.IV.sum()\n",
    "    IV_dict[col.replace(\"GRP_\", \"WOE_\")] = df_groupby.IV.sum()\n",
    "    # print(col, df_groupby.IV.sum())\n",
    "    # print(df_groupby)\n",
    "\n",
    "sorted_cols_by_IV = sorted(IV_dict.items(), key=lambda x: -x[1])\n",
    "selected_cols = [x[0] for x in sorted_cols_by_IV if x[1] > 0.1]\n",
    "selected_cols = [x for x in selected_cols if x not in [\"Out_of_Country\", \"WOE_PD_Total_Scorecard_Points\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2.3: Multivariate Screening\n",
    "df_var_cluster = df.loc[:, selected_cols]\n",
    "var_clusterer = VarClusHi(df_var_cluster, maxeigval2=2)\n",
    "var_clusterer.varclus()\n",
    "print(var_clusterer.info)\n",
    "print(var_clusterer.rsquare)\n",
    "df_rsquare_ratio = var_clusterer.rsquare\n",
    "df_rsquare_ratio[\"IV\"] = df_rsquare_ratio[\"Variable\"].map(IV_dict)\n",
    "cols_max_IV = df_rsquare_ratio[df_rsquare_ratio.groupby(['Cluster'])[\"IV\"].transform(max) == df_rsquare_ratio[\"IV\"]].Variable.to_numpy()\n",
    "cols_min_ratio = df_rsquare_ratio[df_rsquare_ratio.groupby(['Cluster'])[\"RS_Ratio\"].transform(min) == df_rsquare_ratio[\"RS_Ratio\"]].Variable.to_numpy()\n",
    "candidate_cols = list(cols_max_IV) + list(cols_min_ratio)\n",
    "candidate_cols = sorted(list(set(candidate_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.8: Sampling Methodology (more convenient to split after processing)\n",
    "df[\"cluster_num\"] = cluster_labels\n",
    "df_test = df[df[\"TIME_KEY\"] == 15436].reset_index(drop=True)\n",
    "df_intime = df[df[\"TIME_KEY\"] != 15436].reset_index(drop=True)\n",
    "dfs_test = [df_test[df_test.cluster_num == i].reset_index(drop=True) for i in range(5)]\n",
    "dfs = [df_intime[df_intime.cluster_num == i].reset_index(drop=True) for i in range(5)]\n",
    "dfs_train, dfs_val = [None for _ in range(5)], [None for _ in range(5)]\n",
    "for i in range(5):\n",
    "    dfs_train[i], dfs_val[i], _, _ = train_test_split(dfs[i], dfs[i][\"target\"], stratify=dfs[i][\"target\"], test_size=0.2, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.3: Model Fitting\n",
    "models = []\n",
    "final_cols = []\n",
    "for i in range(5):\n",
    "    print(f\"model {i}:\")\n",
    "    curr_cols = candidate_cols\n",
    "    model = sm.Logit(dfs_train[i].target, dfs_train[i].loc[:, curr_cols]).fit(disp=False)\n",
    "    min_bic = model.bic\n",
    "    while True:\n",
    "        best_model = None\n",
    "        best_bic = np.inf\n",
    "        best_col = None\n",
    "        for subset in list(itertools.combinations(curr_cols, len(curr_cols)-1)):\n",
    "            reg = sm.Logit(dfs_train[i].target, dfs_train[i].loc[:, subset]).fit(disp=False)\n",
    "            if reg.bic < best_bic:\n",
    "                best_bic = reg.bic\n",
    "                best_model = reg\n",
    "                best_col = subset\n",
    "        if best_bic < min_bic:\n",
    "            min_bic = best_bic\n",
    "            model = best_model\n",
    "            curr_cols = best_col\n",
    "        else:\n",
    "            break\n",
    "    models.append(model)\n",
    "    final_cols.append(curr_cols)\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.4: Scorecard Scaling\n",
    "factor = 20 / np.log(2)\n",
    "offset = 600 - factor*np.log(50)\n",
    "scorecards = []\n",
    "for i in range(5):\n",
    "    GRP_cols = [col.replace(\"WOE_\", \"GRP_\") for col in final_cols[i]]\n",
    "    GRP_dict = dict(df[GRP_cols].nunique())\n",
    "    n = len(final_cols[i])  # number of explanatory variables in the model\n",
    "    scorecard_temps = []\n",
    "    for col in final_cols[i]:\n",
    "        scorecard_temp = pd.DataFrame(np.arange(1, GRP_dict[col.replace(\"WOE_\", \"GRP_\")]+1), columns=['GRP'])\n",
    "        WOE_dict = df[[col.replace(\"WOE_\", \"GRP_\"), col]].set_index(col.replace(\"WOE_\", \"GRP_\")).to_dict()[col]\n",
    "        scorecard_temp[\"WOE\"] = scorecard_temp[\"GRP\"].map(WOE_dict)\n",
    "        scorecard_temp[\"score\"] = offset/n - factor * scorecard_temp.WOE * models[i].params[col]\n",
    "        scorecard_temp.insert(0, \"variable\", col.replace(\"WOE_\", \"\"))\n",
    "        scorecard_temps.append(scorecard_temp)\n",
    "    scorecard = pd.concat(scorecard_temps).reset_index(drop=True)\n",
    "    scorecard[\"score\"] = scorecard[\"score\"].apply(lambda x: round(x))  # round to the nearest integer for business purposes\n",
    "    scorecards.append(scorecard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.5.1: Rank-Ordering\n",
    "def generate_score(df):\n",
    "    def scoring(row):\n",
    "        cluster_num = row.cluster_num\n",
    "        score = 0\n",
    "        for col in final_cols[cluster_num]:\n",
    "            scorecard = scorecards[cluster_num]\n",
    "            score += scorecard[(scorecard.variable == col.replace(\"WOE_\", \"\")) & (scorecard.GRP == row[col.replace(\"WOE_\", \"GRP_\")])].score.to_numpy()\n",
    "        return score[0]\n",
    "    df[\"score\"] = df.apply(lambda row: scoring(row), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# split by train-val-test sets\n",
    "df_train = pd.concat(dfs_train).reset_index(drop=True)\n",
    "df_val = pd.concat(dfs_val).reset_index(drop=True)\n",
    "# df_test has already been created above\n",
    "df_train_scored = generate_score(df_train.copy())\n",
    "df_val_scored = generate_score(df_val.copy())\n",
    "df_test_scored = generate_score(df_test.copy())\n",
    "\n",
    "# split by time\n",
    "dfs_time_scored = [generate_score(df[df.TIME_KEY == tk].copy()) for tk in time_key_dict]\n",
    "\n",
    "\n",
    "## KS statistic\n",
    "def get_KS(df_scored, scores, plot_title_type):\n",
    "    default_count = [df_scored[(df_scored.target == 1) & (df_scored.score == s)].shape[0] for s in scores]\n",
    "    non_default_count = [df_scored[(df_scored.target == 0) & (df_scored.score == s)].shape[0] for s in scores]\n",
    "    default_cum_dist = np.cumsum(default_count) / np.sum(default_count)\n",
    "    non_default_cum_dist = np.cumsum(non_default_count) / np.sum(non_default_count)\n",
    "    KS = scores[np.argmax([d - nd for d, nd in zip(default_cum_dist, non_default_cum_dist)])]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    KS_index = list(scores).index(KS)\n",
    "    ax.plot(scores, default_cum_dist, label=\"default\")\n",
    "    ax.plot(scores, non_default_cum_dist, label=\"non-default\")\n",
    "    ax.axvline(x=KS, color='r', label=f\"KS = {(default_cum_dist[KS_index] - non_default_cum_dist[KS_index]):.4f} at score = {KS}\", linestyle=\"dashed\")\n",
    "    ax.set(xlabel=\"score\", ylabel='% population')\n",
    "    ax.set_title(f\"Cumulative Distributions of Defaults and Non-Defaults of {plot_title_type} Set\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'media/KS_{plot_title_type.lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scores = np.arange(230, 720)\n",
    "get_KS(df_train_scored, scores, \"Training\")\n",
    "get_KS(df_val_scored, scores, \"Validation\")\n",
    "get_KS(df_test_scored, scores, \"Test\")\n",
    "for i, tk in enumerate(time_key_dict):\n",
    "    get_KS(dfs_time_scored[i], scores, time_key_dict[tk])\n",
    "\n",
    "\n",
    "## accuracy ratio\n",
    "def get_AR(df_scored, plot_title_type):\n",
    "    sorted_scores = [y for _, y in sorted(zip(df_scored.score.to_numpy(), df_scored.target.to_numpy()), reverse=False)]\n",
    "    total_default = df_scored.target.sum()\n",
    "    yy = np.append([0], np.cumsum(sorted_scores))\n",
    "    xx = np.arange(0, df_scored.shape[0]+1)\n",
    "    \n",
    "    a = auc([0, df_scored.shape[0]], [0, total_default])  # area under the random baseline\n",
    "    aP = auc([0, total_default, df_scored.shape[0]], [0, total_default, total_default]) - a  # area between the perfect and random baseline\n",
    "    aR = auc(xx, yy) - a  # area between the trained model and the random baseline\n",
    "    AR = aR / aP\n",
    "    print(f\"accuracy ratio (AR) of the model: {AR:.4f}\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0, df_scored.shape[0]], [0, total_default], label=\"random baseline\")\n",
    "    ax.plot(xx, yy, label=\"logistic regression\")\n",
    "    ax.plot([0, total_default, df_scored.shape[0]], [0, total_default, total_default], label=\"perfect baseline\")\n",
    "    ax.set(xlabel=\"total number of observations\", ylabel='number of defaults')\n",
    "    ax.set_title(f\"Cumulative Accuracy Profile of {plot_title_type} Set\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'media/AR_{plot_title_type.lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "get_AR(df_train_scored, \"Training\")\n",
    "get_AR(df_val_scored, \"Validation\")\n",
    "get_AR(df_test_scored, \"Test\")\n",
    "for i, tk in enumerate(time_key_dict):\n",
    "    get_AR(dfs_time_scored[i], time_key_dict[tk])\n",
    "\n",
    "\n",
    "# lift curve\n",
    "def get_lift(df_scored, plot_title_type):\n",
    "    sorted_scores = [y for _, y in sorted(zip(df_scored.score.to_numpy(), df_scored.target.to_numpy()), reverse=False)]\n",
    "    total_default = df_scored.target.sum()\n",
    "    xx = np.arange(1, df_scored.shape[0]+1) / df_scored.shape[0]\n",
    "    yy = (np.cumsum(sorted_scores) / total_default) / xx\n",
    "    print(f\"lift at 10% of the model: {yy[int(df_scored.shape[0] / 10) - 1]}\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0, 1], [1, 1], label=\"random baseline\", linestyle='dashed')\n",
    "    ax.plot(xx, yy, label=\"logistic regression\")\n",
    "    ax.set(xlabel=\"% population\", ylabel='lift')\n",
    "    ax.set_title(f\"Lift Curve of {plot_title_type} Set\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'media/lift_{plot_title_type.lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "get_lift(df_train_scored, \"Training\")\n",
    "get_lift(df_val_scored, \"Validation\")\n",
    "get_lift(df_test_scored, \"Test\")\n",
    "for i, tk in enumerate(time_key_dict):\n",
    "    get_lift(dfs_time_scored[i], time_key_dict[tk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.5.2: Population Stability\n",
    "bins = np.arange(220, 795, 75)\n",
    "buckets = [f\"{x}-{x+75}\" for x in np.arange(220, 720, 75)]\n",
    "df_psi = pd.DataFrame(buckets, columns=[\"bin\"])\n",
    "for i, tk in enumerate(time_key_dict):\n",
    "    df_psi[time_key_dict[tk]] = df_psi[\"bin\"].map(dict(pd.cut(dfs_time_scored[i][\"score\"], bins, labels=buckets).value_counts())) / dfs_time_scored[i].shape[0]\n",
    "# df_psi[\"training\"] = df_psi[\"bin\"].map(dict(pd.cut(df_train_scored[\"score\"], bins, labels=buckets).value_counts())) / df_train_scored.shape[0]\n",
    "# df_psi[\"validation\"] = df_psi[\"bin\"].map(dict(pd.cut(df_val_scored[\"score\"], bins, labels=buckets).value_counts())) / df_val_scored.shape[0]\n",
    "# df_psi[\"test\"] = df_psi[\"bin\"].map(dict(pd.cut(df_test_scored[\"score\"], bins, labels=buckets).value_counts())) / df_test_scored.shape[0]\n",
    "\n",
    "\n",
    "def get_PSI(df, col_x, col_y):\n",
    "    df_psi_temp = (df[col_x] - df[col_y]) * np.log(df[col_x] / df[col_y])\n",
    "    return df_psi_temp.sum()\n",
    "\n",
    "\n",
    "for col_x, col_y in itertools.combinations(time_key_dict.keys(), 2):\n",
    "    col_x, col_y = time_key_dict[col_x], time_key_dict[col_y]\n",
    "    print(f\"PSI between {col_x} and {col_y}: {get_PSI(df_psi, col_x, col_y)}\")\n",
    "\n",
    "df_psi = df_psi.melt(id_vars=[\"bin\"]).rename(columns={\"variable\": \"time\", \"value\": \"distribution\"})\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x='bin', y='distribution', hue='time', data=df_psi, ax=ax)\n",
    "ax.set(xlabel=\"score range\", ylabel='% population')\n",
    "ax.set_title(f\"Score Distributions at Various Time Points\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('media/psi.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.5.3: Scorecard Benchmarking\n",
    "def make_passthrough_mapper(cols):\n",
    "    return DataFrameMapper([([col], CategoricalDomain()) for col in cols])\n",
    "\n",
    "\n",
    "def make_classifier(max_depth=5):\n",
    "    config = {\n",
    "        \"max_depth\" : max_depth\n",
    "    }\n",
    "    return CHAIDClassifier(config = config)\n",
    "\n",
    "\n",
    "pipeline = PMMLPipeline(\n",
    "    [\n",
    "        (\"mapper\", make_passthrough_mapper(candidate_cols)),\n",
    "        (\"classifier\", make_classifier(max_depth=5))\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(df_train.loc[:, candidate_cols], df_train.target)\n",
    "sklearn2pmml(pipeline, \"CHAIDIris.pmml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in new_cols:\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.hist(df[col], bins=20)\n",
    "#     ax.set(xlabel=col, ylabel='frequency')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e943ea8c514027e6881bf1155933aebd182fd7ddad024e25d1449d89c649c50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
